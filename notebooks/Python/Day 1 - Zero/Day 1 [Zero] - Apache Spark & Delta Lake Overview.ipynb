{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2e8d7f-25c8-40b7-a5d5-803abd076921",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Nota importante!\n",
    "\n",
    "Este notebook funcionou com uma versão 12.2 LTS Runtime!\n",
    "\n",
    "Não há preocupações quanto ao tamanho do cluster (memória e núcleos).\n",
    "\n",
    "O objetivo deste notebook é apenas mostrar a versão dos comandos SQL para Python, **sempre use a versão SQL como referência**, como foi o usado durante o curso Databricks SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b13f06-7406-45fb-bd5a-10bf65f4c8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Comandos mágicos do Databricks Notebook\n",
    "\n",
    "| Comando | Resultado |\n",
    "|--------|--------|\n",
    "| `%lsmagic` | Lista todos os comandos mágicos. |\n",
    "| `%md` | Célula como um Markdown. |\n",
    "| `%sql` | UsarSQL. |\n",
    "| `%python` | Utilize Python. |\n",
    "| `%scala` | Usar Scala. |\n",
    "| `%r` | Usar R. |\n",
    "| `%run` | Executa um arquivo Python ou outro notebook. |\n",
    "| `%who` | Mostra todas as variáveis. |\n",
    "| `%env` | Permite inserir variáveis ​​de ambiente. |\n",
    "| `%fs` | Permite usar comandos do sistema de arquivos dos utilitários DBFD. |\n",
    "| `%sh` | Executa comandos Shell no cluster. |\n",
    "| `%matplotlib` | Recursos de back-end do Matplotlib. |\n",
    "| `%config` | Pode definir configurações para notebook. |\n",
    "| `%pip` | Instale pacotes Python. |\n",
    "| `%load` | Carrega o conteúdo de um arquivo em uma célula. |\n",
    "| `%reload` | Recarrega o conteúdo. |\n",
    "| `%jobs` | Lista trabalhos em execução. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d856967-1ca1-48c4-997c-4fc78f9b5acb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## O que é Apache Spark?\n",
    "\n",
    "Apache Spark é um mecanismo de processamento capaz de analisar dados usando SQL, Python, Scala, R e Java. Também possui estruturas para permitir aprendizado de máquina, processamento de gráficos ou streaming:\n",
    "\n",
    "![Spark Engines](https://github.com/tiluan/trn-databricks-sql-zero-to-hero/blob/main/images/spark-engine.png?raw=true)\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "* A API DataFrames funciona devido a uma abstração acima dos RDDs, porém ganhando algo em torno de 5 a 20x em relação aos RDDs tradicionais com seu Catalyst Optimizer.\n",
    "\n",
    "![Spark Unified Engine](https://github.com/tiluan/trn-databricks-sql-zero-to-hero/blob/main/images/unified-engine.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ceb361a-4856-4340-bdb5-4e1763d50eb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Arquitetura Spark Cluster: Drivers, Executores, Slots e Tarefas\n",
    "![Spark Physical Cluster, slots](https://github.com/tiluan/trn-databricks-sql-zero-to-hero/blob/main/images/spark-driver.png?raw=true)\n",
    "\n",
    "Ao criar um cluster, você pode escolher entre **nó único** ou **vários nós**, onde deve incluir o tipo de máquina do driver e também o tipo de máquina de trabalho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c36967e-0113-4396-aa2a-fe49343ba29f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Spark Jobs, avaliação preguiçosa, transformações e ações\n",
    "![Spark Jobs, Lazy Evaluation, Transformations & Actions](https://github.com/tiluan/trn-databricks-sql-zero-to-hero/blob/main/images/spark_bookclub.png?raw=true)\n",
    "\n",
    "\n",
    "* Toda execução do Apache Spark é classificada como Spark Job, que posteriormente é dividida em etapas que irão agrupar tarefas (menor unidade de execução de trabalho).\n",
    "* Como o Apache Spark é anexado ao modelo Lazy Evaluation, cada processo pode ser classificado em **ação** OU **transformação**.\n",
    "* Cada Spark Job é gerado por um comando de ação, enquanto os estágios serão organizados devido a funções de consulta embaralhadas e de otimização.\n",
    "* Embaralhar? Sim, as transformações Spark também podem ser divididas em: **transformações estreitas** e **transformações amplas (shuffle)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f074292c-f8ec-46af-a0c3-b6809bab442c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Alguns exemplos de Spark Action\n",
    "\n",
    "| Comando | Resultado |\n",
    "|--------|--------|\n",
    "| `collect()` | Retorna uma matriz que contém todas as linhas deste conjunto de dados. |\n",
    "| `count()` | Retorna o número de linhas no conjunto de dados. |\n",
    "| `first()` | Retorna a primeira linha. |\n",
    "| `foreach(f)` | Aplica uma função f a todas as linhas. |\n",
    "| `head()` | Retorna a primeira linha. |\n",
    "| `show(..)` | Exibe as 20 principais linhas do conjunto de dados em formato tabular. |\n",
    "| `take(n)` | Retorna as primeiras n linhas do conjunto de dados. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc93755-5274-45f6-823a-b78460c44c21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Alguns exemplos de transformação Spark\n",
    "\n",
    "| Comando | Tipo | Resultado |\n",
    "|--------|--------|-------------|\n",
    "| `filter()` | Estreito | Retorna um novo DataFrame após aplicar a função de filtro no conjunto de dados de origem. |\n",
    "| `distinct()` | Largo | Retorna um novo DataFrame contendo as linhas distintas neste DataFrame. |\n",
    "| `join()` | Largo | Junta-se a outro DataFrame, usando a expressão de junção fornecida. |\n",
    "| `union()` | Estreito | Combina dois DataFrames e retorna o novo DataFrame. |\n",
    "| `repartition()` | Largo | Retorna um novo DataFrame (particionado por hash) particionado pelas expressões de particionamento fornecidas. |\n",
    "| `groupBy()` | Largo | Agrupa o DataFrame usando as colunas especificadas, para que possamos executar a agregação nelas. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9727e761-3643-4f4b-8fd9-dbeffc8147b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## O que é Delta Lake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f62dfc5-ef7f-4213-bee8-592acb93fb39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Delta Lake](https://github.com/tiluan/trn-databricks-sql-zero-to-hero/blob/main/images/delta-lake.png?raw=true)\n",
    "\n",
    "* Delta Lake é uma estrutura de armazenamento de código aberto que permite construir uma arquitetura Lakehouse com mecanismos de computação, atuando como um **formato de tabela nativo em Databricks**, usando Parquet como formato físico para anexar todos os dados.\n",
    "* As tabelas Delta são criadas com base nas **garantias ACID** fornecidas pelo protocolo Delta Lake de código aberto. ACID significa atomicidade, consistência, isolamento e durabilidade.\n",
    "* O Delta Lake pode implementar as propriedades ACID devido aos **controles de log de transações** salvos durante **cada confirmação** nos dados. Durante uma transação, os arquivos de dados são gravados no diretório de arquivos que suporta a tabela. Quando a transação for concluída, uma nova entrada será confirmada no log de transações que inclui os caminhos para todos os arquivos gravados durante a transação. Cada commit incrementa a versão da tabela e torna novos arquivos de dados visíveis para operações de leitura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0a7f4e-21c8-4ed1-bfd8-5bebbf0d4cc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criando uma tabela Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95da6880-47b2-42eb-b93f-973dd0196603",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/databricks-datasets/learning-spark-v2/people/people-10m.delta\")\n",
    "table_name = \"people_10millions\"\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2912fe9b-ff8c-4995-a5d5-c06ac6176a21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('dbfs:/databricks-datasets/learning-spark-v2/people/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba567652-0f54-434c-94a9-d271b9f4f81a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lendo uma tabela Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1f648a-5cf9-4a38-a502-256818f6694c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_people = spark.read.table(\"default.people_10millions\")\n",
    "display(df_people.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f72ee40c-29b8-4ff2-8b97-e925f7b9291e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls 'dbfs:/databricks-datasets/learning-spark-v2/people/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457e667a-ecf9-4a7f-91f5-a952af687aee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_people.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e543be89-de34-4ac3-82fd-0faaecb90985",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Obtendo detalhes sobre a tabela Delta (propriedades, tamanho, localização, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e654e3-2d5f-47ec-96bc-b805a5f2d823",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_people.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08cd1d3b-1a25-476f-a9e0-836e83461093",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_people = spark.read.format(\"delta\").table(\"people_10millions\")\n",
    "df_people.createOrReplaceTempView(\"people_10millions\")\n",
    "display(spark.sql(\"DESCRIBE TABLE EXTENDED people_10millions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be01d846-ddcc-43e6-9a45-b86049025467",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inspecionando o histórico da tabela Delta (criação, upserts, exclusões, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3866ce3-ac94-4b54-a6e3-929976c35ef9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE DETAIL people_10millions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74caf889-ebdd-479c-8ecb-f8b8a36c6aae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY hive_metastore.default.people_10millions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b830b0-238e-428c-872d-0f1a6d7648dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Alterando Tabela Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98aca128-58c4-491c-b37d-3be0010de3c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Inserindo novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dbf4ce7-dcca-4abf-b803-99fcab16c35d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_people = df_people.withColumn(\"gender\", when(df_people.gender == \"M\", \"H\")\n",
    "                                        .when(df_people.gender == \"F\", \"M\")\n",
    "                                        .otherwise(df_people.gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae067ce-9dd1-42c6-802e-b2542ba64708",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Removendo linhas/valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e408b80f-f0bb-4921-9fc8-125f46a1f640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_people.delete(\"birthDate >= '2000-01-01'\")\n",
    "# DELETE FROM hive_metastore.default.people_10millions WHERE birthDate >= '2000-01-01'\n",
    "\n",
    "df_people = df_people.filter(\"birthDate < '2000-01-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bfdf467-ed47-4b33-a61b-f15da87c3bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_people.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ec47a6-43ec-4ef5-b625-7664869b989f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inspecionando novamente o histórico da tabela Delta (criação, upserts, exclusões, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e51e6a-6704-479b-8159-619e8f25812e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY hive_metastore.default.people_10millions;\n",
    "\n",
    "-- display(df_people.history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51c12a7-6da5-4a7a-911c-806d54bb89d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Resgatando através da Viagem no Tempo para recuperar a versão antiga da Tabela Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e6f3db-510a-4155-848e-bdecd4e1a9e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--SELECT * FROM hive_metastore.default.people_10millions VERSION AS OF 0\n",
    "CREATE OR REPLACE TABLE hive_metastore.default.people_10millions AS SELECT * FROM hive_metastore.default.people_10millions VERSION AS OF 2\n",
    "\n",
    "-- spark.sql(\"CREATE OR REPLACE TABLE hive_metastore.default.people_10millions AS SELECT * FROM hive_metastore.default.people_10millions VERSION AS OF 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e42425-f019-4f17-98b7-e1956f68c204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_people.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a1b4be-13f6-4dcf-aa32-c28d17fdaf2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_people.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac57822f-ce40-48a1-96d3-3143159132bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('dbfs:/user/hive/warehouse/people_10millions/_delta_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79bf847d-5282-4284-a346-09a06ec11967",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.json('/user/hive/warehouse/people_10millions/_delta_log/00000000000000000003.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09b920f-c950-4efc-ad17-9df1d0f3e28a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Limpando e se livrando da história"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794f39db-8906-4fce-ae7f-a5899270f5f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "VACUUM hive_metastore.default.people_10millions\n",
    "-- df_people.vacuum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b1d331-c05f-4f28-bc18-20ebf6c419bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(1) FROM hive_metastore.default.people_10millions\n",
    "\n",
    "--display(df_people.count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4142316991058816,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 1 [Zero] - Apache Spark & Delta Lake Overview",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
